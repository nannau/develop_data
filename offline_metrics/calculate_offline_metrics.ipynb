{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xarray as xr\n",
    "from pkg_resources import resource_filename\n",
    "from sklearn.decomposition import PCA\n",
    "from DoWnGAN.losses import content_MSELoss, content_loss, SSIM_Loss\n",
    "from DoWnGAN.losses import content_loss, content_MSELoss, SSIM_Loss\n",
    "\n",
    "from DoWnGAN.dataloader import xr_standardize_field\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from DoWnGAN.prep_gan import xr_standardize_field, dt_index, filter_times\n",
    "from torch.multiprocessing import Pool, Process, set_start_method\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "from matplotlib import colors\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "plt.style.use(['science','ieee','no-latex'])\n",
    "experiment_number = 0\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "from dask.distributed import Client\n",
    "print(\"Cores:\", multiprocessing.cpu_count())\n",
    "client = Client(n_workers = multiprocessing.cpu_count(), memory_limit=\"6GB\")\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "runs = {\"florida\":\n",
    "        {\n",
    "#             \"hash_code_CNN_L1\": 'e25c6b40324643c3afc1cf42981b11b5',\n",
    "#             \"hash_code_CNN_MSSSIM\": 'd7d73e34e60c46ff96c8a49e5e9e973b',\n",
    "            \"hash_code_13x13\": 'e1d15a0615ca489aa6a17ec60247d0af',\n",
    "            \"hash_code_9x9\": '3f48868c52404eb0a833897aa4642871',\n",
    "            \"hash_code_5x5\": '1824682ae27c48669665cf042052d584',\n",
    "            \"hash_code_nf\": 'feda42500d2b45549be96f1bf62b0b03'\n",
    "        },\n",
    "       \"central\":\n",
    "        {\n",
    "#             \"hash_code_CNN_L1\": 'fbe44b0423204805bc6af4d7d6ac562e',\n",
    "            \"hash_code_13x13\": 'bcf7e7cfa8ab4c4196ad6a2bb18e8601',\n",
    "            \"hash_code_9x9\": '079a94c41ad3482996cc2b9f95adba8d',\n",
    "            \"hash_code_5x5\": '202ea9f8a73b401fa22e62c24d9ab2d0',\n",
    "            \"hash_code_nf\": '0c5ee480663f4f9eb7200f8879aa1244'\n",
    "        },\n",
    "        \"west\":\n",
    "        {\n",
    "#             \"hash_code_CNN_L1\": 'f76c0170818244629de4544805f93a59',\n",
    "            \"hash_code_13x13\": 'c4ec13e65fe74b399fc9e325a9966fef',\n",
    "            \"hash_code_9x9\": '6abe7a9940c04b47819689070100e5e6',\n",
    "            \"hash_code_5x5\": '70f5be887eff42e8a216780752644b2f',\n",
    "            \"hash_code_nf\": 'db9f0fae83c949eaad5d1176a43dae47'\n",
    "        },\n",
    "        \n",
    "}\n",
    "filters = {\n",
    "    \"hash_code_13x13\": nn.AvgPool2d(13, stride=1, padding=0),\n",
    "    \"hash_code_9x9\": nn.AvgPool2d(9, stride=1, padding=0),\n",
    "    \"hash_code_5x5\": nn.AvgPool2d(5, stride=1, padding=0),\n",
    "    \"hash_code_nf\": lambda x: x\n",
    "}\n",
    "\n",
    "padding = {\n",
    "    \"hash_code_13x13\": nn.ReplicationPad2d(6),\n",
    "    \"hash_code_9x9\": nn.ReplicationPad2d(4),\n",
    "    \"hash_code_5x5\": nn.ReplicationPad2d(2),\n",
    "    \"hash_code_nf\": lambda x: x\n",
    "}\n",
    "\n",
    "\n",
    "region = \"central\"\n",
    "set = \"train\"\n",
    "\n",
    "def standardize_field(x, m, s):\n",
    "    return (x-m)/s\n",
    "\n",
    "with torch.no_grad():\n",
    "    for set in [\"validation\", \"train\"]:\n",
    "        for region in runs.keys():\n",
    "            print(region)\n",
    "            sf = 8\n",
    "            region_area = {\n",
    "                \"florida\": (4, 20, 70, 86),\n",
    "                \"central\": (30, 46, 50, 66),\n",
    "                \"west\": (30, 46, 15, 31)\n",
    "            }\n",
    "\n",
    "            hash_list = [runs[region][key] for key in runs[region].keys()]\n",
    "\n",
    "            low, up, l, r = region_area[region]\n",
    "            fine_paths = {\n",
    "                \"U\": resource_filename(\"DoWnGAN\", \"data/wrf/U10_regrid_16/regrid_16_6hrly_wrf2d_d01_ctrl_U10*.nc\"),\n",
    "                \"V\": resource_filename(\"DoWnGAN\", \"data/wrf/V10_regrid_16/regrid_16_6hrly_wrf2d_d01_ctrl_V10*.nc\"),\n",
    "            }\n",
    "\n",
    "            u10 = xr.open_mfdataset(glob.glob(fine_paths[\"U\"])).U10\n",
    "            v10 = xr.open_mfdataset(glob.glob(fine_paths[\"V\"])).V10\n",
    "\n",
    "\n",
    "            # Extract times in datetime format\n",
    "            times = dt_index(u10.Times)\n",
    "\n",
    "            # Apply filter to times for months you'd like\n",
    "            time_mask = filter_times(times, mask_years=[2000, 2006, 2010])\n",
    "\n",
    "            if set == \"validation\":\n",
    "                time_mask = ~time_mask.copy()\n",
    "    #             time_mask = ~time_mask\n",
    "            time_mask[0] = False\n",
    "\n",
    "\n",
    "            path = f\"ground_truth/{region}_{set}_fine_written.nc\"\n",
    "            path_train = f\"ground_truth/{region}_train_fine_written.nc\"\n",
    "            if os.path.exists(path) and os.path.exists(path_train):\n",
    "                dsgt = xr.open_dataset(f\"ground_truth/{region}_{set}_fine_written.nc\")\n",
    "                u10_patch = dsgt[\"u10\"]\n",
    "                v10_patch = dsgt[\"v10\"]\n",
    "                dsgt_train = xr.open_dataset(f\"ground_truth/{region}_train_fine_written.nc\")\n",
    "            else:\n",
    "                raise ValueError(\"No processed ground truth data found!\")\n",
    "\n",
    "            mu = float(dsgt_train.u10.mean())\n",
    "            mv = float(dsgt_train.v10.mean())\n",
    "            su = float(dsgt_train.u10.std())\n",
    "            sv = float(dsgt_train.v10.std())\n",
    "\n",
    "            print(\"U10 Mean, std\", mu, su)\n",
    "            print(\"v10 Mean, std\", mv, sv)\n",
    "\n",
    "\n",
    "            dsgt = xr.Dataset()\n",
    "            u10_patch = standardize_field(u10_patch, mu, su)\n",
    "            v10_patch = standardize_field(v10_patch, mv, sv)\n",
    "            dsgt[\"u10\"] = u10_patch\n",
    "            dsgt[\"v10\"] = v10_patch\n",
    "\n",
    "            print(\"U10 Mean, std\", dsgt[\"u10\"].mean(), dsgt[\"u10\"].std())\n",
    "            print(\"v10 Mean, std\", dsgt[\"v10\"].mean(), dsgt[\"v10\"].std())\n",
    "\n",
    "            coarse_paths = {\n",
    "                \"UV\": resource_filename(\"DoWnGAN\", \"./data/interim_2000-10-01_to_2013-09-30.nc\")\n",
    "            }\n",
    "\n",
    "            # Load ERA Interim\n",
    "            coarse = xr.open_dataset(coarse_paths[\"UV\"], engine=\"scipy\").astype(\"float\")\n",
    "            # Organize lats in increasing order:\n",
    "            coarse = coarse.sortby(\"latitude\", ascending=True).rename({\"longitude\":\"lon\", \"latitude\":\"lat\"})\n",
    "\n",
    "            if set == \"validation\":\n",
    "                coarse_u10 = coarse.u10[time_mask, low:up, l:r]\n",
    "                coarse_v10 = coarse.v10[time_mask, low:up, l:r]\n",
    "                coarse_u10_train = coarse.u10[~time_mask, low:up, l:r]\n",
    "                coarse_v10_train = coarse.v10[~time_mask, low:up, l:r]\n",
    "                cmu, cmv = coarse_u10_train.mean(), coarse_v10_train.mean()\n",
    "                csu, csv = coarse_u10_train.std(), coarse_v10_train.std()\n",
    "                coarse_u10 = standardize_field(coarse_u10, cmu, csu)\n",
    "                coarse_v10 = standardize_field(coarse_v10, cmv, csv)\n",
    "            else:  \n",
    "                coarse_u10 = coarse.u10[time_mask, low:up, l:r]\n",
    "                coarse_v10 = coarse.v10[time_mask, low:up, l:r]\n",
    "\n",
    "                coarse_u10 = xr_standardize_field(coarse_u10)#.chunk({\"time\": 250})\n",
    "                coarse_v10 = xr_standardize_field(coarse_v10)#.chunk({\"time\": 250})\n",
    "\n",
    "            randoms = np.random.randint(0, dsgt.Times.shape[0], dsgt.Times.shape[0])\n",
    "\n",
    "            fine_t = torch.stack([\n",
    "                torch.from_numpy(dsgt.u10[randoms, ...].values).to(\"cpu\"),\n",
    "                torch.from_numpy(dsgt.v10[randoms, ...].values).to(\"cpu\")\n",
    "            ], dim=1)\n",
    "\n",
    "\n",
    "            train_coarse_sp = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/train/{region}_surface_pressure.nc\")\n",
    "            train_coarse_mask = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/train/{region}_land_sea_mask.nc\")\n",
    "            train_coarse_sf = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/train/{region}_surface_friction.nc\")\n",
    "            train_coarse_cape = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/train/{region}_cape.nc\")\n",
    "            train_coarse_geo = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/train/{region}_geopotential_height.nc\")\n",
    "            train_cov_list = [coarse_u10, coarse_v10, train_coarse_sp.to_array()[0, ...], train_coarse_sf.to_array()[0, ...], train_coarse_geo.to_array()[0, ...], train_coarse_cape.to_array()[0, ...]]\n",
    "\n",
    "            means = [x.mean() for x in train_cov_list]\n",
    "            std = [x.std() for x in train_cov_list]\n",
    "\n",
    "            # THE PROBLEM HAS TO DO WITH THE NORMALIZATION OF THESE COVARIATES!!!!!!!!!!!!!!!!! :O :O :O :O :O \n",
    "            coarse_sp = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/{set}/{region}_surface_pressure.nc\")\n",
    "            coarse_mask = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/{set}/{region}_land_sea_mask.nc\")\n",
    "            coarse_sf = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/{set}/{region}_surface_friction.nc\")\n",
    "            coarse_cape = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/{set}/{region}_cape.nc\")\n",
    "            coarse_geo = xr.open_dataset(f\"/home/nannau/msc/netcdf_regions_gt/organized/covariates/{set}/{region}_geopotential_height.nc\")\n",
    "\n",
    "            cov_list = [coarse_u10, coarse_v10, coarse_sp.to_array()[0, ...], coarse_sf.to_array()[0, ...], coarse_geo.to_array()[0, ...], coarse_cape.to_array()[0, ...]]\n",
    "\n",
    "            # Do not add in this extra normalization step\n",
    "            cov_list = [standardize_field(x, means[i], std[i]) for i, x in enumerate(cov_list)]\n",
    "            torch_list = [torch.from_numpy(ds.values) for ds in cov_list]\n",
    "\n",
    "            torch_list.insert(2, torch.from_numpy(coarse_mask.to_array()[0, ...].values))\n",
    "            coarse_t_loaded = torch.stack(torch_list, dim=1).float()[randoms, ...]\n",
    "\n",
    "            aranged = torch.split(torch.arange(0, coarse_t_loaded.size(0)), int(coarse_t_loaded.size(0)/(64)), dim=0)\n",
    "            print(\"Chunk size: \", aranged[0].size())\n",
    "\n",
    "\n",
    "            print(\"Coarse: \", coarse_t_loaded.size())\n",
    "            print(\"Fine: \", fine_t.size())\n",
    "\n",
    "            complete_hash_dict = {}\n",
    "    #         for hc in hash_list:\n",
    "            for f in runs[region].keys():\n",
    "                hc = runs[region][f]\n",
    "                long_mae = []\n",
    "                long_mse = []\n",
    "                long_wass =[]\n",
    "                long_msssim = []\n",
    "                hash_dict = {}\n",
    "                for i in tqdm_notebook(range(1000)):\n",
    "                    if i % 1 == 0:\n",
    "                        logged_model_g = f'/home/nannau/msc/Fall_2021/DoWnGAN/DoWnGAN/mlflow_experiments/{experiment_number}/{hc}/artifacts/Generator/Generator_{i}/'\n",
    "                        G = mlflow.pytorch.load_model(logged_model_g)\n",
    "                        state_dict = mlflow.pytorch.load_state_dict(logged_model_g)\n",
    "                        G.load_state_dict(state_dict)\n",
    "\n",
    "                        logged_model_c = f'/home/nannau/msc/Fall_2021/DoWnGAN/DoWnGAN/mlflow_experiments/{experiment_number}/{hc}/artifacts/Critic/Critic_{i}/'\n",
    "                        C = mlflow.pytorch.load_model(logged_model_c)\n",
    "                        state_dict = mlflow.pytorch.load_state_dict(logged_model_c)\n",
    "                        C.load_state_dict(state_dict)\n",
    "\n",
    "                        \n",
    "                        lmae = []\n",
    "                        lmse = []\n",
    "                        lmsssim = []\n",
    "                        lwass = []\n",
    "            #             for fchunk, cchunk in zip(torch.split(fine_t, N_chunks, dim=0), torch.split(coarse_t_loaded, N_chunks, dim=0)):\n",
    "                        for r in aranged:\n",
    "            #             for _ in range(100):\n",
    "            #                 r = np.random.randint(0, coarse_t_loaded.size(0), bsize)\n",
    "            #                 Y = G(cchunk.to(device).float())\n",
    "                            Y = G(coarse_t_loaded[r, ...].to(device).float())\n",
    "                            X = fine_t[r, ...].to(device)\n",
    "\n",
    "                            if f != \"hash_code_nf\":\n",
    "                                C_real = torch.mean(C(X - filters[f](padding[f](X)).to(device)))\n",
    "                                C_fake = torch.mean(C(Y - filters[f](padding[f](Y)).to(device)))\n",
    "                                wass = C_real - C_fake\n",
    "\n",
    "                            else:\n",
    "                                C_real = torch.mean(C(X))\n",
    "                                C_fake = torch.mean(C(Y))\n",
    "                                wass = C_real - C_fake\n",
    "                            lwass.append(wass.detach().cpu())\n",
    "\n",
    "                            Y[:, 0, ...] = Y[:, 0, ...]*su + mu\n",
    "                            Y[:, 1, ...] = Y[:, 1, ...]*sv + mv\n",
    "\n",
    "                            X[:, 0, ...] = X[:, 0, ...]*su + mu\n",
    "                            X[:, 1, ...] = X[:, 1, ...]*sv + mv\n",
    "\n",
    "            #                 assert Y.size() == fchunk.size()\n",
    "                            mae = content_loss(\n",
    "                                X.to(device),\n",
    "            #                     fchunk.to(device),\n",
    "                                Y.to(device),\n",
    "                                device\n",
    "                            ).item()\n",
    "                            lmae.append(mae)\n",
    "\n",
    "                            mse = content_MSELoss(\n",
    "                                X.to(device),\n",
    "            #                     fchunk.to(device),\n",
    "                                Y.to(device),\n",
    "                                device\n",
    "                            ).item()\n",
    "                            lmse.append(mse)\n",
    "\n",
    "                            msssim = SSIM_Loss(\n",
    "                                X.to(device),\n",
    "            #                     fchunk.to(device),\n",
    "                                Y.to(device),\n",
    "                                device\n",
    "                            ).item()\n",
    "                            lmsssim.append(msssim)\n",
    "                        del G\n",
    "                        del state_dict\n",
    "                        long_mae.append((np.min(lmae), np.mean(lmae), np.max(lmae)))\n",
    "                        long_mse.append((np.min(lmse), np.mean(lmse), np.max(lmse)))\n",
    "                        long_msssim.append((np.min(lmsssim), np.mean(lmsssim), np.max(lmsssim)))\n",
    "                        long_wass.append((np.min(lwass), np.mean(lwass), np.max(lwass)))\n",
    "\n",
    "\n",
    "                hash_dict[\"mae\"] = long_mae\n",
    "                hash_dict[\"mse\"] = long_mse\n",
    "                hash_dict[\"msssim\"] = long_msssim\n",
    "                hash_dict[\"wasserstein\"] = long_wass\n",
    "\n",
    "                complete_hash_dict[hc] = hash_dict\n",
    "\n",
    "            df = pd.DataFrame()\n",
    "            metric = [\"mae\", \"mse\", \"msssim\", \"wasserstein\"]\n",
    "            for m in metric:\n",
    "                for key in complete_hash_dict.keys():\n",
    "                    df[f\"{key}_{m}_min\"] = [t[0] for t in complete_hash_dict[key][m]]\n",
    "                    df[f\"{key}_{m}_mean\"] = [t[1] for t in complete_hash_dict[key][m]]\n",
    "                    df[f\"{key}_{m}_max\"] = [t[2] for t in complete_hash_dict[key][m]]\n",
    "\n",
    "            # for key in complete_hash_dict.keys():\n",
    "            #     print(key)\n",
    "            df.to_csv(f\"{region}_{set}_offline_metrics.csv\")\n",
    "            del fine_t\n",
    "            del coarse_t_loaded\n",
    "            torch.cuda.empty_cache()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cores: 16\n",
      "florida\n",
      "Masking these years: [2000, 2006, 2010]\n",
      "U10 Mean, std -1.7083028554916382 4.247805118560791\n",
      "v10 Mean, std -0.440981388092041 4.205788612365723\n",
      "U10 Mean, std <xarray.DataArray 'u10' ()>\n",
      "array(0.08260679, dtype=float32) <xarray.DataArray 'u10' ()>\n",
      "array(1.0378276, dtype=float32)\n",
      "v10 Mean, std <xarray.DataArray 'v10' ()>\n",
      "array(-0.05707378, dtype=float32) <xarray.DataArray 'v10' ()>\n",
      "array(1.0098454, dtype=float32)\n",
      "Chunk size:  torch.Size([51])\n",
      "Coarse:  torch.Size([3287, 7, 16, 16])\n",
      "Fine:  torch.Size([3287, 2, 128, 128])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf54d224f4ba4e3597d29bbe561a5979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c92f71598ab4537acaced3de0a30b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112e24f5256d4663a3dce5604f359820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fe300c025841979acc80832ad79735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "central\n",
      "Masking these years: [2000, 2006, 2010]\n",
      "U10 Mean, std 0.505122721195221 2.814021110534668\n",
      "v10 Mean, std 0.07797596603631973 3.2136151790618896\n",
      "U10 Mean, std <xarray.DataArray 'u10' ()>\n",
      "array(-0.01178362, dtype=float32) <xarray.DataArray 'u10' ()>\n",
      "array(1.009194, dtype=float32)\n",
      "v10 Mean, std <xarray.DataArray 'v10' ()>\n",
      "array(-0.0005574, dtype=float32) <xarray.DataArray 'v10' ()>\n",
      "array(0.9753815, dtype=float32)\n",
      "Chunk size:  torch.Size([51])\n",
      "Coarse:  torch.Size([3287, 7, 16, 16])\n",
      "Fine:  torch.Size([3287, 2, 128, 128])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6477e6f5014dc2bbe5e61e0e620f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959a1b41be914c96b136ad4a61cf205f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571ade21309d4f9e97828a51088cf123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6016ddcb44684a3e91aa5b210827aae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "west\n",
      "Masking these years: [2000, 2006, 2010]\n",
      "U10 Mean, std 0.9174613952636719 3.011970281600952\n",
      "v10 Mean, std -0.1184549406170845 3.958988666534424\n",
      "U10 Mean, std <xarray.DataArray 'u10' ()>\n",
      "array(-0.0471669, dtype=float32) <xarray.DataArray 'u10' ()>\n",
      "array(1.029438, dtype=float32)\n",
      "v10 Mean, std <xarray.DataArray 'v10' ()>\n",
      "array(0.06888004, dtype=float32) <xarray.DataArray 'v10' ()>\n",
      "array(1.0693603, dtype=float32)\n",
      "Chunk size:  torch.Size([51])\n",
      "Coarse:  torch.Size([3287, 7, 16, 16])\n",
      "Fine:  torch.Size([3287, 2, 128, 128])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293ae01e42df4b9bb14e5233d8d11d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d01869bce646869c43323450682f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2eb040b10f5477d84ba82e908677358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3d88db333f49a58274fb100ced1c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "florida\n",
      "Masking these years: [2000, 2006, 2010]\n",
      "U10 Mean, std -1.7083028554916382 4.247805118560791\n",
      "v10 Mean, std -0.440981388092041 4.205788612365723\n",
      "U10 Mean, std <xarray.DataArray 'u10' ()>\n",
      "array(7.249134e-08, dtype=float32) <xarray.DataArray 'u10' ()>\n",
      "array(1.0000006, dtype=float32)\n",
      "v10 Mean, std <xarray.DataArray 'v10' ()>\n",
      "array(-2.3660812e-07, dtype=float32) <xarray.DataArray 'v10' ()>\n",
      "array(1.0000015, dtype=float32)\n",
      "Chunk size:  torch.Size([245])\n",
      "Coarse:  torch.Size([15704, 7, 16, 16])\n",
      "Fine:  torch.Size([15704, 2, 128, 128])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857be6bca22749f68cdd8c5239816a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067c4383e37a4344a61661dfef5dbe1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26334e73d6fc4127be33e9bb8eab3494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666e2c54e27a4c44ac3516fb079d4663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "central\n",
      "Masking these years: [2000, 2006, 2010]\n",
      "U10 Mean, std 0.505122721195221 2.814021110534668\n",
      "v10 Mean, std 0.07797596603631973 3.2136151790618896\n",
      "U10 Mean, std <xarray.DataArray 'u10' ()>\n",
      "array(1.2905484e-07, dtype=float32) <xarray.DataArray 'u10' ()>\n",
      "array(0.99999875, dtype=float32)\n",
      "v10 Mean, std <xarray.DataArray 'v10' ()>\n",
      "array(1.6700231e-09, dtype=float32) <xarray.DataArray 'v10' ()>\n",
      "array(0.99999726, dtype=float32)\n",
      "Chunk size:  torch.Size([245])\n",
      "Coarse:  torch.Size([15704, 7, 16, 16])\n",
      "Fine:  torch.Size([15704, 2, 128, 128])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04272c9059284f21a48263b497f0fc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}